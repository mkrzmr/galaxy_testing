{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "publication = (GALAXY_INPUTS['publication']).lower()\n",
    "year = (GALAXY_INPUTS['year'])\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Removes characters that are illegal in filenames.\"\"\"\n",
    "    return re.sub(r'[<>:\"/\\\\ |?*]', '_', name)\n",
    "\n",
    "def zip_and_put_to_galaxy(year, source_folder):\n",
    "    \"\"\"\n",
    "    Zips the folder, sends it to Galaxy history, and cleans up raw files.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Export to Galaxy ---\")\n",
    "    \n",
    "    # 1. Zip the folder\n",
    "    zip_base_name = f\"downloads_{year}\" # shutil adds .zip automatically\n",
    "    print(f\"Zipping folder: {source_folder}...\")\n",
    "    \n",
    "    try:\n",
    "        # shutil.make_archive(base_name, format, root_dir)\n",
    "        zip_path = shutil.make_archive(zip_base_name, 'zip', source_folder)\n",
    "        print(f\"Archive created: {zip_path}\")\n",
    "        \n",
    "        # 2. Send to Galaxy\n",
    "        print(\"Sending zip file to Galaxy History...\")\n",
    "        put(f\"{zip_base_name}.zip\")\n",
    "        print(\"Successfully sent to History.\")\n",
    "\n",
    "        # 3. Clean up (Delete raw files to save space)\n",
    "        print(\"Cleaning up raw files...\")\n",
    "        shutil.rmtree(source_folder)\n",
    "        print(\"Raw files deleted successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during export/cleanup: {e}\")\n",
    "\n",
    "def extract_date_from_label(label):\n",
    "    \"\"\"\n",
    "    Tries to find a date in YYYY-MM-DD format within the label string.\n",
    "    Returns the date string or 'unknown_date' if not found.\n",
    "    \"\"\"\n",
    "    # Look for pattern YYYY-MM-DD\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', label)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"unknown_date\"\n",
    "\n",
    "def download_newspaper_year(year):\n",
    "    base_folder = f\"outputs/collection/downloads_{year}\"\n",
    "    \n",
    "    # 1. Construct the Collection URL\n",
    "    collection_url = f\"https://iiif.onb.ac.at/presentation/collection/{publication}_{year}\"\n",
    "    print(f\"Fetching Collection: {collection_url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(collection_url)\n",
    "        response.raise_for_status()\n",
    "        collection_data = response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching collection: {e}\")\n",
    "        return\n",
    "\n",
    "    if \"manifests\" not in collection_data:\n",
    "        print(\"No manifests found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(collection_data['manifests'])} issues. Starting download...\\n\")\n",
    "\n",
    "    # 2. Iterate through Manifests (Issues)\n",
    "    for i, manifest_entry in enumerate(collection_data[\"manifests\"]):\n",
    "        manifest_url = manifest_entry.get(\"@id\")\n",
    "        manifest_label = manifest_entry.get(\"label\", f\"Issue_{i}\")\n",
    "        \n",
    "        # --- NEW: Extract Date for Filename ---\n",
    "        issue_date = extract_date_from_label(manifest_label)\n",
    "        \n",
    "        # Create a clean folder name for this sequence/issue\n",
    "        safe_folder_name = sanitize_filename(manifest_label)\n",
    "        issue_path = os.path.join(base_folder, safe_folder_name)\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(issue_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"[{i+1}] Processing: {manifest_label} (Date: {issue_date})\")\n",
    "\n",
    "        try:\n",
    "            m_response = requests.get(manifest_url)\n",
    "            m_data = m_response.json()\n",
    "            \n",
    "            # 3. Iterate through Sequences -> Canvases (Pages)\n",
    "            sequences = m_data.get(\"sequences\", [])\n",
    "            for sequence in sequences:\n",
    "                canvases = sequence.get(\"canvases\", [])\n",
    "                \n",
    "                print(f\"    - Found {len(canvases)} pages.\")\n",
    "                \n",
    "                for canvas in canvases:\n",
    "                    # Get the page label (usually \"0001\", \"0002\", etc.)\n",
    "                    # We strip to remove whitespace and zfill to ensure correct sorting (e.g., 001)\n",
    "                    raw_page_label = str(canvas.get(\"label\", \"0\")).strip()\n",
    "                    clean_page_label = raw_page_label.zfill(4) \n",
    "                    \n",
    "                    # --- NEW: Construct Filename with Date ---\n",
    "                    # Format: 1756-01-03_page_0001.jpg\n",
    "                    filename = f\"{issue_date}_page_{clean_page_label}.jpg\"\n",
    "                    \n",
    "                    # --- FIX: Join folder + filename ---\n",
    "                    file_path = os.path.join(issue_path, filename)\n",
    "\n",
    "                    # Skip if already downloaded\n",
    "                    if os.path.exists(file_path):\n",
    "                        # print(f\"      [Skipping] {filename} exists.\") # Optional: Reduce clutter\n",
    "                        continue\n",
    "\n",
    "                    # Find the image URL\n",
    "                    images = canvas.get(\"images\", [])\n",
    "                    for image in images:\n",
    "                        resource = image.get(\"resource\", {})\n",
    "                        image_url = resource.get(\"@id\")\n",
    "                        \n",
    "                        if image_url:\n",
    "                            print(f\"      Downloading {filename}...\", end=\"\\r\")\n",
    "                            try:\n",
    "                                img_r = requests.get(image_url, stream=True)\n",
    "                                img_r.raise_for_status()\n",
    "                                with open(file_path, 'wb') as f:\n",
    "                                    for chunk in img_r.iter_content(chunk_size=8192):\n",
    "                                        f.write(chunk)\n",
    "                            except Exception as e:\n",
    "                                print(f\"\\n      Error downloading {image_url}: {e}\")\n",
    "                            \n",
    "                            time.sleep(0.2)\n",
    "                print(\"\") \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error processing manifest {manifest_url}: {e}\")\n",
    "\n",
    "        # Sleep between issues\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    print(\"\\nAll downloads complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_newspaper_year(year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
